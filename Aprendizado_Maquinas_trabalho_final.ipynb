{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyrtonPinheiro/Aprendizado_Maquina-Tarefa04/blob/main/Aprendizado_Maquinas_trabalho_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "qJle8Eb4IDd6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHmD79jTGmnr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e7a8d96-a606-47bd-a2f6-f65a637320e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files from GitHub repo to Colab...\n",
            "Finished!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('config.py', 'wb').write(r.content)\n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "\n",
        "from config import *\n",
        "config_chapter7()\n",
        "# This is needed to render the plots in this chapter\n",
        "from plots.chapter7 import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stepbystep.v3 import StepByStep"
      ],
      "metadata": {
        "id": "9o4N8Ge0HSu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
        "from torchvision.transforms.v2 import Compose, ToImage, Normalize,  \\\n",
        "Resize, ToPILImage, CenterCrop, RandomResizedCrop, ToDtype, ToTensor\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import alexnet, resnet18, inception_v3\n",
        "\n",
        "# Updated for Torchvision 0.15\n",
        "from torchvision.models.alexnet import AlexNet_Weights\n",
        "from torchvision.models.inception import Inception_V3_Weights\n",
        "from torchvision.models.resnet import ResNet18_Weights\n",
        "# from torchvision.models.alexnet import model_urls\n",
        "\n",
        "import os\n",
        "import shutil"
      ],
      "metadata": {
        "id": "tBHQyB0uH4Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "nlIM5_tfIGd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"gpiosenka/cards-image-datasetclassification\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJS676zEJus6",
        "outputId": "aed89781-a630-433c-c001-289a5587a418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/gpiosenka/cards-image-datasetclassification?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 385M/385M [00:03<00:00, 117MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/gpiosenka/cards-image-datasetclassification/versions/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the destination folder if it doesn't exist\n",
        "rps_folder = os.path.join(\"/content/\", \"rps\")\n",
        "os.makedirs(rps_folder, exist_ok=True)\n",
        "\n",
        "# Iterate through the subfolders in the train folder\n",
        "train_folder = os.path.join(path, \"train\")\n",
        "\n",
        "for folder_name in os.listdir(train_folder):\n",
        "    source_folder = os.path.join(train_folder, folder_name)\n",
        "    destination_folder = os.path.join(rps_folder, folder_name)\n",
        "\n",
        "    # Check if it's a directory before moving\n",
        "    if os.path.isdir(source_folder):\n",
        "        shutil.move(source_folder, destination_folder)"
      ],
      "metadata": {
        "id": "ZheWGIO8J-Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rps_folder = os.path.join(\"/content/\", \"rps-test-set\")\n",
        "os.makedirs(rps_folder, exist_ok=True)\n",
        "\n",
        "# Iterate through the subfolders in the train folder\n",
        "train_folder = os.path.join(path, \"test\")\n",
        "\n",
        "for folder_name in os.listdir(train_folder):\n",
        "    source_folder = os.path.join(train_folder, folder_name)\n",
        "    destination_folder = os.path.join(rps_folder, folder_name)\n",
        "\n",
        "    # Check if it's a directory before moving\n",
        "    if os.path.isdir(source_folder):\n",
        "        shutil.move(source_folder, destination_folder)"
      ],
      "metadata": {
        "id": "XjpJM3SFURwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ImageNet statistics\n",
        "normalizer = Normalize( mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "\n",
        "composer = Compose([Resize(224),\n",
        "                    CenterCrop(224),\n",
        "                    ToImage(),\n",
        "                    ToDtype(torch.float32, scale=True),\n",
        "                    normalizer])\n",
        "\n",
        "train_data = ImageFolder(root='rps', transform=composer)\n",
        "val_data = ImageFolder(root='rps-test-set', transform=composer)\n",
        "\n",
        "# Builds a loader of each set\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=16)"
      ],
      "metadata": {
        "id": "NpzpuFyoHclv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Finetuning"
      ],
      "metadata": {
        "id": "lW3pyM-UIK_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# repeating import from the top for reference\n",
        "from torchvision.models.resnet import ResNet18_Weights\n",
        "\n",
        "# UPDATED\n",
        "###########################################################\n",
        "# This is the recommended way of loading a pretrained\n",
        "# model's weights\n",
        "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "# model = resnet18(pretrained=True)\n",
        "###########################################################\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model.fc = nn.Linear(512, 53)"
      ],
      "metadata": {
        "id": "Q1uZYK8FHfGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7050548-3d2a-4bb6-992a-9aebcbd5d87c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 188MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
        "optimizer_model = optim.Adam(model.parameters(), lr=3e-4)\n",
        "sbs_transfer = StepByStep(model, multi_loss_fn, optimizer_model)"
      ],
      "metadata": {
        "id": "JgatIS6qHhCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sbs_transfer.set_loaders(train_loader, val_loader)\n",
        "sbs_transfer.train(1)"
      ],
      "metadata": {
        "id": "HsWufUy7HijJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "StepByStep.loader_apply(val_loader, sbs_transfer.correct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMO1M-mrHj0V",
        "outputId": "b69d4f35-be7a-4e04-8c09-50f75e7da94d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [2, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [4, 5],\n",
              "        [4, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [4, 5],\n",
              "        [5, 5],\n",
              "        [3, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [4, 5],\n",
              "        [5, 5],\n",
              "        [4, 5],\n",
              "        [5, 5],\n",
              "        [3, 5],\n",
              "        [3, 5],\n",
              "        [5, 5],\n",
              "        [4, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [4, 5],\n",
              "        [4, 5],\n",
              "        [4, 5],\n",
              "        [5, 5],\n",
              "        [4, 5],\n",
              "        [5, 5],\n",
              "        [4, 5],\n",
              "        [4, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [5, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Feature Extractor"
      ],
      "metadata": {
        "id": "wRnK4K2alT4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_model(model):\n",
        "  for parameter in model.parameters():\n",
        "    parameter.requires_grad = False"
      ],
      "metadata": {
        "id": "5BxBMJCuY1yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# repeating import from the top for reference\n",
        "from torchvision.models.resnet import ResNet18_Weights\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# UPDATED\n",
        "###########################################################\n",
        "model = resnet18(weights=ResNet18_Weights.DEFAULT).to(device)\n",
        "# model = resnet18(pretrained=True).to(device)\n",
        "###########################################################\n",
        "\n",
        "model.fc = nn.Identity()\n",
        "freeze_model(model)"
      ],
      "metadata": {
        "id": "g--omQaRHlWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c3dc9a6-5d17-4500-8a4e-d0f01c830a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 186MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessed_dataset(model, loader, device=None):\n",
        "  if device is None:\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "  features = None\n",
        "  labels = None\n",
        "\n",
        "  for i, (x, y) in enumerate(loader):\n",
        "    model.eval()\n",
        "    output = model(x.to(device))\n",
        "    if i == 0:\n",
        "      features = output.detach().cpu()\n",
        "      labels = y.cpu()\n",
        "    else:\n",
        "      features = torch.cat([features, output.detach().cpu()])\n",
        "      labels = torch.cat([labels, y.cpu()])\n",
        "\n",
        "  dataset = TensorDataset(features, labels)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "3lKYf6SgZRgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_preproc = preprocessed_dataset(model, train_loader, device)\n",
        "val_preproc = preprocessed_dataset(model, val_loader, device)\n",
        "train_preproc_loader = DataLoader(train_preproc, batch_size=16, shuffle=True)\n",
        "val_preproc_loader = DataLoader(val_preproc, batch_size=16)"
      ],
      "metadata": {
        "id": "jJEEZwrSHnKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "top_model = nn.Sequential(nn.Linear(512, 53))\n",
        "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
        "optimizer_top = optim.Adam(top_model.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "JWkgGJZMHojA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sbs_top = StepByStep(top_model, multi_loss_fn, optimizer_top)\n",
        "sbs_top.set_loaders(train_preproc_loader, val_preproc_loader)\n",
        "sbs_top.train(100)"
      ],
      "metadata": {
        "id": "aAQvy_2HHp-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "StepByStep.loader_apply(val_preproc_loader, sbs_top.correct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oycLX0rHrru",
        "outputId": "dc2cccd5-eecb-418c-e726-33495ed6966e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4, 5],\n",
              "        [4, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [4, 5],\n",
              "        [1, 5],\n",
              "        [4, 5],\n",
              "        [3, 5],\n",
              "        [2, 5],\n",
              "        [2, 5],\n",
              "        [2, 5],\n",
              "        [2, 5],\n",
              "        [2, 5],\n",
              "        [4, 5],\n",
              "        [2, 5],\n",
              "        [2, 5],\n",
              "        [4, 5],\n",
              "        [1, 5],\n",
              "        [2, 5],\n",
              "        [2, 5],\n",
              "        [0, 5],\n",
              "        [5, 5],\n",
              "        [4, 5],\n",
              "        [1, 5],\n",
              "        [2, 5],\n",
              "        [2, 5],\n",
              "        [1, 5],\n",
              "        [3, 5],\n",
              "        [4, 5],\n",
              "        [3, 5],\n",
              "        [0, 5],\n",
              "        [3, 5],\n",
              "        [4, 5],\n",
              "        [2, 5],\n",
              "        [1, 5],\n",
              "        [2, 5],\n",
              "        [3, 5],\n",
              "        [1, 5],\n",
              "        [2, 5],\n",
              "        [1, 5],\n",
              "        [2, 5],\n",
              "        [3, 5],\n",
              "        [2, 5],\n",
              "        [3, 5],\n",
              "        [3, 5],\n",
              "        [2, 5],\n",
              "        [5, 5],\n",
              "        [3, 5],\n",
              "        [3, 5],\n",
              "        [2, 5],\n",
              "        [2, 5],\n",
              "        [1, 5],\n",
              "        [3, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc = top_model\n",
        "sbs_temp = StepByStep(model, None, None)"
      ],
      "metadata": {
        "id": "3eFoE3f5HteS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "StepByStep.loader_apply(val_loader, sbs_temp.correct)\n"
      ],
      "metadata": {
        "id": "KDmgN5RjHvkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69be38d1-0c72-4df0-cc26-02ee60145cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4, 5],\n",
              "        [4, 5],\n",
              "        [5, 5],\n",
              "        [5, 5],\n",
              "        [4, 5],\n",
              "        [1, 5],\n",
              "        [4, 5],\n",
              "        [3, 5],\n",
              "        [2, 5],\n",
              "        [2, 5],\n",
              "        [2, 5],\n",
              "        [2, 5],\n",
              "        [2, 5],\n",
              "        [4, 5],\n",
              "        [2, 5],\n",
              "        [2, 5],\n",
              "        [4, 5],\n",
              "        [1, 5],\n",
              "        [2, 5],\n",
              "        [2, 5],\n",
              "        [0, 5],\n",
              "        [5, 5],\n",
              "        [4, 5],\n",
              "        [1, 5],\n",
              "        [2, 5],\n",
              "        [2, 5],\n",
              "        [1, 5],\n",
              "        [3, 5],\n",
              "        [4, 5],\n",
              "        [3, 5],\n",
              "        [0, 5],\n",
              "        [3, 5],\n",
              "        [4, 5],\n",
              "        [2, 5],\n",
              "        [1, 5],\n",
              "        [2, 5],\n",
              "        [3, 5],\n",
              "        [1, 5],\n",
              "        [2, 5],\n",
              "        [1, 5],\n",
              "        [2, 5],\n",
              "        [3, 5],\n",
              "        [2, 5],\n",
              "        [3, 5],\n",
              "        [3, 5],\n",
              "        [2, 5],\n",
              "        [5, 5],\n",
              "        [3, 5],\n",
              "        [3, 5],\n",
              "        [2, 5],\n",
              "        [2, 5],\n",
              "        [1, 5],\n",
              "        [3, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}